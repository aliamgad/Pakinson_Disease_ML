# -*- coding: utf-8 -*-
"""ml_3rd_assignment_ds.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CiZzYQT4fYYZxPym9yNVRuij8LoknPai
"""

import pandas as pd
import numpy as np
import time # Import time for measuring training/testing duration
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif, chi2 # Import chi2 for categorical feature selection
from sklearn.ensemble import RandomForestClassifier # For embedded feature importance and as a model
from sklearn.linear_model import LogisticRegression # Another model
from sklearn.svm import SVC # Another model
from xgboost import XGBClassifier # Another model
from sklearn.ensemble import VotingClassifier # Import VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score # More evaluation metrics
from ast import literal_eval # For parsing nested lists
import joblib # For saving the pipeline
import matplotlib.pyplot as plt # For visualization
import seaborn as sns # For enhanced visualization
from sklearn.base import BaseEstimator, TransformerMixin # For creating custom transformer

# --- Google Drive Mount (Colab Only) ---
# This section is for Google Colab environments to access files from Drive.
# If running locally, comment out or remove this section and ensure the CSV path is correct.
try:
    from google.colab import drive
    print("Mounting Google Drive...")
    # Mount the user's Drive at /content/drive to access the dataset
    drive.mount('/content/drive')
    # Define the path to the dataset on Google Drive
    dataset_path = '/content/drive/My Drive/parkinsons_disease_data_cls.csv'
except ImportError:
    print("Google Colab environment not detected. Assuming local execution.")
    # Define the local path to the dataset
    dataset_path = 'parkinsons_disease_data_cls.csv' # Adjust this path if your file is elsewhere

# Custom Transformer for Feature Engineering and Parsing
class FeatureEngineerAndParser(BaseEstimator, TransformerMixin):
    def __init__(self):
        # Store unique values for one-hot encoding during fit
        self.medical_history_categories_ = None
        self.symptoms_categories_ = None
        # Store median/mean values for imputation/normalization if needed within engineering
        # (Currently relying on SimpleImputer in the pipeline, but could add here)
        self.age_median_ = None
        self.updrs_median_ = None
        self.moca_median_ = None
        self.func_median_ = None
        self.ldl_median_ = None
        self.hdl_median_ = None
        self.total_chol_median_ = None
        self.systolic_median_ = None
        self.diastolic_median_ = None
        self.diet_median_ = None
        self.sleep_median_ = None
        self.weekly_activity_median_ = None
        self.diet_max_ = None
        self.sleep_max_ = None
        self.weekly_activity_max_ = None


    def fit(self, X, y=None):
        X_copy = X.copy() # Work on a copy

        # 1. Convert 'WeeklyPhysicalActivity (hr)' to hours (float)
        if 'WeeklyPhysicalActivity (hr)' in X_copy.columns:
            def time_string_to_hours(time_str):
                if pd.isnull(time_str):
                    return np.nan
                try:
                    parts = str(time_str).split(':')
                    if len(parts) == 2:
                        hours = int(parts[0])
                        minutes = int(parts[1])
                        return hours + minutes / 60
                    elif len(parts) == 1:
                         return int(parts[0])
                    else:
                        return np.nan
                except ValueError:
                    return np.nan
            X_copy.loc[:, 'WeeklyPhysicalActivity (hr)'] = X_copy['WeeklyPhysicalActivity (hr)'].apply(time_string_to_hours)
            # Store median for imputation if needed later (though SimpleImputer handles this)
            self.weekly_activity_median_ = X_copy['WeeklyPhysicalActivity (hr)'].median()
            self.weekly_activity_max_ = X_copy['WeeklyPhysicalActivity (hr)'].max()


        # 2. Parse nested features and store categories for OHE
        if 'MedicalHistory' in X_copy.columns:
            # Safely parse and convert each item to string, handling potential NaNs
            parsed_medhist = X_copy['MedicalHistory'].apply(
                lambda x: [str(item) for item in literal_eval(x)] if pd.notnull(x) and isinstance(x, str) else []
            )
            all_medhist_items = [item for sublist in parsed_medhist for item in sublist]
            self.medical_history_categories_ = sorted(list(set(all_medhist_items)))

        if 'Symptoms' in X_copy.columns:
            # Safely parse and convert each item to string, handling potential NaNs
            parsed_symptoms = X_copy['Symptoms'].apply(
                lambda x: [str(item) for item in literal_eval(x)] if pd.notnull(x) and isinstance(x, str) else []
            )
            all_symptoms_items = [item for sublist in parsed_symptoms for item in sublist]
            self.symptoms_categories_ = sorted(list(set(all_symptoms_items)))

        # 3. Store medians/means for engineered features that might need imputation
        # (These will be used in transform if SimpleImputer is not used for these specific cols)
        # For robustness, we still rely on SimpleImputer in the pipeline, but storing these is good practice
        # if you were to implement imputation directly here.
        if 'Age' in X_copy.columns: self.age_median_ = X_copy['Age'].median()
        if 'UPDRS' in X_copy.columns: self.updrs_median_ = X_copy['UPDRS'].median()
        if 'MoCA' in X_copy.columns: self.moca_median_ = X_copy['MoCA'].median()
        if 'FunctionalAssessment' in X_copy.columns: self.func_median_ = X_copy['FunctionalAssessment'].median()
        if 'CholesterolLDL' in X_copy.columns: self.ldl_median_ = X_copy['CholesterolLDL'].median()
        if 'CholesterolHDL' in X_copy.columns: self.hdl_median_ = X_copy['CholesterolHDL'].median()
        if 'CholesterolTotal' in X_copy.columns: self.total_chol_median_ = X_copy['CholesterolTotal'].median()
        if 'SystolicBP' in X_copy.columns: self.systolic_median_ = X_copy['SystolicBP'].median()
        if 'DiastolicBP' in X_copy.columns: self.diastolic_median_ = X_copy['DiastolicBP'].median()
        if 'DietQuality' in X_copy.columns:
            self.diet_median_ = X_copy['DietQuality'].median()
            self.diet_max_ = X_copy['DietQuality'].max()
        if 'SleepQuality' in X_copy.columns:
            self.sleep_median_ = X_copy['SleepQuality'].median()
            self.sleep_max_ = X_copy['SleepQuality'].max()


        return self

    def transform(self, X):
        X_copy = X.copy() # Work on a copy

        # 1. Convert 'WeeklyPhysicalActivity (hr)' to hours (float)
        if 'WeeklyPhysicalActivity (hr)' in X_copy.columns:
            def time_string_to_hours(time_str):
                 if pd.isnull(time_str):
                    return np.nan
                 try:
                    parts = str(time_str).split(':')
                    if len(parts) == 2:
                        hours = int(parts[0])
                        minutes = int(parts[1])
                        return hours + minutes / 60
                    elif len(parts) == 1:
                         return int(parts[0])
                    else:
                        return np.nan
                 except ValueError:
                    return np.nan
            X_copy.loc[:, 'WeeklyPhysicalActivity (hr)'] = X_copy['WeeklyPhysicalActivity (hr)'].apply(time_string_to_hours)


        # 2. Parse nested features and create OHE columns
        medhist_df = pd.DataFrame(index=X_copy.index)
        if 'MedicalHistory' in X_copy.columns and self.medical_history_categories_ is not None:
            # Safely parse and convert each item to string before exploding and getting dummies
            parsed_medhist = X_copy['MedicalHistory'].apply(
                lambda x: [str(item) for item in literal_eval(x)] if pd.notnull(x) and isinstance(x, str) else []
            )
            exploded = parsed_medhist.explode().fillna('missing_category_placeholder') # Fill NaNs after explode with a placeholder string
            dummies = pd.get_dummies(exploded, prefix='MedicalHistory')
            # Ensure all categories seen during fit are present, fill missing with 0
            for cat in self.medical_history_categories_:
                 col_name = f'MedicalHistory_{cat}'
                 if col_name not in dummies.columns:
                     dummies[col_name] = 0
            # Drop the placeholder column if it was created
            if 'MedicalHistory_missing_category_placeholder' in dummies.columns:
                 dummies = dummies.drop(columns='MedicalHistory_missing_category_placeholder')
            dummies = dummies.groupby(level=0).sum()
            medhist_df = dummies[sorted([col for col in dummies.columns if col.startswith('MedicalHistory_')])] # Keep consistent order


        symptom_df = pd.DataFrame(index=X_copy.index)
        if 'Symptoms' in X_copy.columns and self.symptoms_categories_ is not None:
            # Safely parse and convert each item to string before exploding and getting dummies
            parsed_symptoms = X_copy['Symptoms'].apply(
                lambda x: [str(item) for item in literal_eval(x)] if pd.notnull(x) and isinstance(x, str) else []
            )
            exploded = parsed_symptoms.explode().fillna('missing_category_placeholder') # Fill NaNs after explode with a placeholder string
            dummies = pd.get_dummies(exploded, prefix='Symptoms')
            # Ensure all categories seen during fit are present, fill missing with 0
            for cat in self.symptoms_categories_:
                 col_name = f'Symptoms_{cat}'
                 if col_name not in dummies.columns:
                     dummies[col_name] = 0
             # Drop the placeholder column if it was created
            if 'Symptoms_missing_category_placeholder' in dummies.columns:
                 dummies = dummies.drop(columns='Symptoms_missing_category_placeholder')
            dummies = dummies.groupby(level=0).sum()
            symptom_df = dummies[sorted([col for col in dummies.columns if col.startswith('Symptoms_')])] # Keep consistent order


        # Drop original nested columns
        cols_to_drop_nested = ['MedicalHistory', 'Symptoms']
        X_copy = X_copy.drop(columns=cols_to_drop_nested, errors='ignore')

        # Concatenate the new parsed features
        X_copy = pd.concat([X_copy, medhist_df], axis=1)
        X_copy = pd.concat([X_copy, symptom_df], axis=1)


        # 3. Feature engineering by adding some suggested features (knowledge based)
        # Ensure necessary columns exist before creating engineered features
        # Fill NaNs with stored medians/means or 0 before calculations if needed
        # (Though SimpleImputer later handles NaNs, doing it here makes engineered features valid)

        # Ensure motor_symptoms exist and fill potential NaNs with 0 for sum
        motor_symptoms = ['Tremor', 'Rigidity', 'Bradykinesia', 'PosturalInstability']
        existing_motor = [c for c in motor_symptoms if c in X_copy.columns]
        for col in existing_motor:
            X_copy.loc[:, col] = pd.to_numeric(X_copy[col], errors='coerce').fillna(0) # Ensure numeric and fill NaNs


        if existing_motor:
            X_copy.loc[:, 'MotorSymptomCount'] = X_copy[existing_motor].sum(axis=1)
        else:
            X_copy.loc[:, 'MotorSymptomCount'] = 0 # Add column even if no motor symptoms exist initially

        # Cognitive–Motor Interaction
        if 'MoCA' in X_copy.columns and 'MotorSymptomCount' in X_copy.columns:
             moca_filled = X_copy['MoCA'].fillna(self.moca_median_ if self.moca_median_ is not None else 0)
             X_copy.loc[:, 'MoCA_MotorSymptomInteraction'] = moca_filled * X_copy['MotorSymptomCount']

        # Age-Related Interactions
        if 'Age' in X_copy.columns:
            age_filled = X_copy['Age'].fillna(self.age_median_ if self.age_median_ is not None else 0)
            if 'UPDRS' in X_copy.columns:
                updrs_filled = X_copy['UPDRS'].fillna(self.updrs_median_ if self.updrs_median_ is not None else 0)
                X_copy.loc[:, 'Age_UPDRS_Interaction'] = age_filled * updrs_filled
            if 'MoCA' in X_copy.columns:
                moca_filled = X_copy['MoCA'].fillna(self.moca_median_ if self.moca_median_ is not None else 0)
                X_copy.loc[:, 'Age_MoCA_Interaction'] = age_filled * moca_filled
            if 'FunctionalAssessment' in X_copy.columns:
                func_filled = X_copy['FunctionalAssessment'].fillna(self.func_median_ if self.func_median_ is not None else 0)
                X_copy.loc[:, 'Age_FunctionalAssessment_Interaction'] = age_filled * func_filled

        # Risk Factor Interactions
        hist_cols = ['FamilyHistoryParkinsons', 'TraumaticBrainInjury']
        existing_hist = [c for c in hist_cols if c in X_copy.columns]
        for col in existing_hist:
             X_copy.loc[:, col] = pd.to_numeric(X_copy[col], errors='coerce').fillna(0)


        if 'MotorSymptomCount' in X_copy.columns and 'FamilyHistoryParkinsons' in X_copy.columns:
            X_copy.loc[:, 'FamilyHistory_MotorSymptom_Interaction'] = X_copy['FamilyHistoryParkinsons'] * X_copy['MotorSymptomCount']
        if 'UPDRS' in X_copy.columns and 'TraumaticBrainInjury' in X_copy.columns:
            updrs_filled = X_copy['UPDRS'].fillna(self.updrs_median_ if self.updrs_median_ is not None else 0)
            X_copy.loc[:, 'TBI_UPDRS_Interaction'] = X_copy['TraumaticBrainInjury'] * updrs_filled


        # Symptom Combinations: ensure columns exist and fill NaNs with 0
        pairs = [('Tremor','Rigidity'), ('Bradykinesia','PosturalInstability'), ('SleepDisorders','Constipation')]
        for a,b in pairs:
            if a in X_copy.columns and b in X_copy.columns:
                col_a_numeric = pd.to_numeric(X_copy[a], errors='coerce').fillna(0)
                col_b_numeric = pd.to_numeric(X_copy[b], errors='coerce').fillna(0)
                X_copy.loc[:, f"{a}_and_{b}"] = col_a_numeric * col_b_numeric


        # Severity Indicators (avoid division by zero)
        if 'Age' in X_copy.columns:
            age_filled = X_copy['Age'].fillna(self.age_median_ if self.age_median_ is not None else 1e-6) # Use a small epsilon if all ages are 0 or NaN
            if 'UPDRS' in X_copy.columns:
                updrs_filled = X_copy['UPDRS'].fillna(self.updrs_median_ if self.updrs_median_ is not None else 0)
                X_copy.loc[:, 'UPDRS_per_Age'] = updrs_filled / (age_filled + 1e-6) # Add epsilon to denominator
            if 'MoCA' in X_copy.columns:
                moca_filled = X_copy['MoCA'].fillna(self.moca_median_ if self.moca_median_ is not None else 0)
                X_copy.loc[:, 'MoCA_per_Age'] = moca_filled / (age_filled + 1e-6) # Add epsilon to denominator

        # Lipid Ratios and Pulse Pressure
        if 'CholesterolLDL' in X_copy.columns and 'CholesterolHDL' in X_copy.columns:
            ldl_filled = X_copy['CholesterolLDL'].fillna(self.ldl_median_ if self.ldl_median_ is not None else 0)
            hdl_filled = X_copy['CholesterolHDL'].fillna(self.hdl_median_ if self.hdl_median_ is not None else 1e-6) # Add epsilon for division
            X_copy.loc[:, 'LDL_to_HDL'] = ldl_filled / (hdl_filled + 1e-6)
        if 'CholesterolTotal' in X_copy.columns and 'CholesterolHDL' in X_copy.columns:
            total_filled = X_copy['CholesterolTotal'].fillna(self.total_chol_median_ if self.total_chol_median_ is not None else 0)
            hdl_filled = X_copy['CholesterolHDL'].fillna(self.hdl_median_ if self.hdl_median_ is not None else 1e-6) # Add epsilon for division
            X_copy.loc[:, 'Total_to_HDL'] = total_filled / (hdl_filled + 1e-6)
        if 'SystolicBP' in X_copy.columns and 'DiastolicBP' in X_copy.columns:
            systolic_filled = X_copy['SystolicBP'].fillna(self.systolic_median_ if self.systolic_median_ is not None else 0)
            diastolic_filled = X_copy['DiastolicBP'].fillna(self.diastolic_median_ if self.diastolic_median_ is not None else 0)
            X_copy.loc[:, 'PulsePressure'] = systolic_filled - diastolic_filled

        # Comorbidity Score for history cols
        comorb_cols_list = ['FamilyHistoryParkinsons','TraumaticBrainInjury','Hypertension','Diabetes','Depression','Stroke']
        existing_comorb = [c for c in comorb_cols_list if c in X_copy.columns]
        if existing_comorb:
            for col in existing_comorb:
                 X_copy.loc[:, col] = pd.to_numeric(X_copy[col], errors='coerce').fillna(0)
            X_copy.loc[:, 'ComorbidityScore'] = X_copy[existing_comorb].sum(axis=1)
        else:
             X_copy.loc[:, 'ComorbidityScore'] = 0 # Add column even if no comorbidity cols exist

        # Cognitive–Motor composite
        if 'MoCA' in X_copy.columns and 'FunctionalAssessment' in X_copy.columns:
            moca_filled = X_copy['MoCA'].fillna(self.moca_median_ if self.moca_median_ is not None else 0)
            func_filled = X_copy['FunctionalAssessment'].fillna(self.func_median_ if self.func_median_ is not None else 0)
            X_copy.loc[:, 'CogMotorInteraction'] = moca_filled * func_filled

        # Health behavior normalization and score
        behavior_cols_list = ['DietQuality','SleepQuality','WeeklyPhysicalActivity (hr)']
        existing_behavior = [c for c in behavior_cols_list if c in X_copy.columns]
        if existing_behavior:
            for c in existing_behavior:
                X_copy.loc[:, c] = pd.to_numeric(X_copy[c], errors='coerce').fillna(getattr(self, f"{c.replace(' (hr)', '').lower()}_median_", 0)) # Use stored median or 0
                max_val = getattr(self, f"{c.replace(' (hr)', '').lower()}_max_", 1) # Use stored max or 1
                X_copy.loc[:, c] = X_copy[c] / (max_val if max_val and not pd.isna(max_val) and max_val != 0 else 1)
            X_copy.loc[:, 'HealthBehaviorScore'] = X_copy[existing_behavior].sum(axis=1)
        else:
             X_copy.loc[:, 'HealthBehaviorScore'] = 0 # Add column even if no behavior cols exist

        return X_copy

# 1. Load Dataset
print("Step 1: Loading Dataset...")
try:
    df = pd.read_csv(dataset_path)
    print("Dataset loaded successfully.")
    print(f"Initial DataFrame shape: {df.shape}")
    print("Columns:", df.columns.tolist())
except FileNotFoundError:
    print(f"Error: Dataset not found at {dataset_path}")
    exit() # Stop execution if the dataset is not found

# Initial data cleaning/preparation based on initial script and description
# Drop identifiers as they are not features
cols_to_drop_initial = ['PatientID', 'DoctorInCharge']
existing_cols_to_drop = [col for col in cols_to_drop_initial if col in df.columns]
if existing_cols_to_drop:
    print(f"Dropping identifier columns: {existing_cols_to_drop}...")
    df.drop(columns=existing_cols_to_drop, inplace=True)
    print(f"DataFrame shape after dropping identifiers: {df.shape}")


# Define target and features before splitting
target = 'Diagnosis'

X = df.drop(columns=[target])
y = df[target].values # Target is now numerical
print(f"Separated target variable '{target}'.")

# 2. Split to train and test
print("\nStep 2: Splitting data into training and testing sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y # Stratify to maintain class distribution
)
print(f"Data split complete. Train shape: {X_train.shape}, Test shape: {X_test.shape}")


# 3. Numerical Feature Visualization and Analysis (Performed on Training Data Only)
print("\nStep 3: Numerical Feature Visualization and Analysis (on Training Data)...")
# Create a temporary training DataFrame for EDA
df_train_eda = X_train.copy()
df_train_eda[target] = y_train # Add target back for context if needed for some plots, but mostly analyze features

numerical_cols_train = df_train_eda.select_dtypes(include=np.number).columns.tolist()
if target in numerical_cols_train: # Exclude target if it was added for context
    numerical_cols_train.remove(target)

print(f"Analyzing {len(numerical_cols_train)} numerical features from the training set: {numerical_cols_train}")

# Display descriptive statistics for numerical features in training data
if numerical_cols_train: # Check if there are any numerical columns
    print("\nDescriptive Statistics for Numerical Features (Training Set):")
    print(df_train_eda[numerical_cols_train].describe().T)

    # Analyze distinct values
    print("\nDistinct Value Counts for Numerical Features (Training Set):")
    for col in numerical_cols_train:
        distinct_count = df_train_eda[col].nunique()
        print(f"- {col}: {distinct_count} distinct values")
        if distinct_count < 20:
            print(f"  (Note: Low distinct count suggests this might be a discrete or categorical-like numerical feature)")

    # Visualizations
    print("\nGenerating visualizations for numerical features (Training Set)...")
    n_cols_viz = 3
    n_rows_viz = (len(numerical_cols_train) + n_cols_viz - 1) // n_cols_viz

    plt.figure(figsize=(n_cols_viz * 5, n_rows_viz * 4))
    for i, col in enumerate(numerical_cols_train):
        plt.subplot(n_rows_viz, n_cols_viz, i + 1)
        sns.histplot(data=df_train_eda, x=col, kde=True)
        plt.title(f'Distribution of {col} (Train)')
        plt.xlabel(col)
        plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(n_cols_viz * 5, n_rows_viz * 4))
    for i, col in enumerate(numerical_cols_train):
        plt.subplot(n_rows_viz, n_cols_viz, i + 1)
        sns.boxplot(data=df_train_eda, y=col) # Changed to y for better display with many boxplots
        plt.title(f'Box Plot of {col} (Train)')
        plt.ylabel(col)
    plt.tight_layout()
    plt.show()
else:
    print("No numerical features found in the training data for EDA.")

print("--- End: Numerical Feature Visualization and Analysis ---")

# --- Start: Manual Hyperparameter Tuning Demonstration ---
# This section now uses the FeatureEngineerAndParser as the first step in the pipeline
print("\n--- Start: Manual Hyperparameter Tuning Demonstration ---")

manual_tuning_results = {}

# Create a base pipeline including the feature engineering/parsing step
base_pipeline_manual = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', 'passthrough'), # Placeholder - will be replaced by the actual preprocessor
    ('classifier', 'passthrough') # Placeholder - will be replaced by the actual classifier
])

# Fit the feature engineer and parser on the training data first to identify columns
fe_parser = FeatureEngineerAndParser()
X_train_processed_manual = fe_parser.fit_transform(X_train)

# Identify column types after feature engineering and parsing for manual tuning pipelines
all_cols_manual = X_train_processed_manual.columns.tolist()
numeric_cols_manual = X_train_processed_manual.select_dtypes(include=np.number).columns.tolist()
categorical_cols_manual = X_train_processed_manual.select_dtypes(include='object').columns.tolist()

# Define the preprocessor for manual tuning pipelines (without selection here)
preprocessor_manual = ColumnTransformer(transformers=[
    ('num', Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), numeric_cols_manual),
    ('cat', Pipeline([('impute', SimpleImputer(strategy='constant', fill_value='missing')), ('ohe', OneHotEncoder(handle_unknown='ignore'))]), categorical_cols_manual)
], remainder='drop')

# --- Logistic Regression Manual Tuning ---
print("\nManual Tuning for Logistic Regression:")
lr_base_pipeline_manual = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', LogisticRegression(random_state=42, solver='liblinear', max_iter=1000))
])
# Parameters to tune manually: C, max_iter
lr_params_manual = {
    'C': [0.1, 1.0, 10.0],
    'max_iter': [100, 500, 1000]
}

lr_tuning_results = {
    'C': {'values': [], 'accuracy': []},
    'max_iter': {'values': [], 'accuracy': []}
}

# Evaluate base LR model
print("  Evaluating base Logistic Regression model (default parameters)...")
lr_base_model = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)
lr_base_pipeline = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', lr_base_model)
])
lr_base_pipeline.fit(X_train, y_train)
y_pred_lr_base = lr_base_pipeline.predict(X_test)
accuracy_lr_base = accuracy_score(y_test, y_pred_lr_base)
f1_lr_base = f1_score(y_test, y_pred_lr_base) # Keep F1 calculation for comparison printout
print(f"  Base LR Accuracy: {accuracy_lr_base:.4f}, Base LR F1-Score: {f1_lr_base:.4f}")


# Tune C (fixing max_iter)
print("  Varying 'C' parameter:")
for c_val in lr_params_manual['C']:
    params = {'classifier__C': c_val, 'classifier__max_iter': 1000} # Fix max_iter
    lr_pipeline_manual = lr_base_pipeline_manual.set_params(**params)
    lr_pipeline_manual.fit(X_train, y_train) # Fit on original X_train
    y_pred_manual = lr_pipeline_manual.predict(X_test) # Predict on original X_test
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    C={c_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    lr_tuning_results['C']['values'].append(c_val)
    lr_tuning_results['C']['accuracy'].append(accuracy_manual)


# Tune max_iter (fixing C)
print("  Varying 'max_iter' parameter:")
for max_iter_val in lr_params_manual['max_iter']:
    params = {'classifier__C': 1.0, 'classifier__max_iter': max_iter_val} # Fix C
    lr_pipeline_manual = lr_base_pipeline_manual.set_params(**params)
    lr_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = lr_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    max_iter={max_iter_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    lr_tuning_results['max_iter']['values'].append(max_iter_val)
    lr_tuning_results['max_iter']['accuracy'].append(accuracy_manual)

# Plotting LR manual tuning results
print("\nPlotting Logistic Regression manual tuning results...")
best_lr_accuracy = accuracy_lr_base

for param, results in lr_tuning_results.items():
    if results['values']: # Check if there are results to plot
        plt.figure(figsize=(8, 5))
        plt.plot(results['values'], results['accuracy'], marker='o', linestyle='-', label='Tuned Accuracy')
        # Add base model performance line
        plt.axhline(y=accuracy_lr_base, color='r', linestyle='--', label='Base Accuracy')

        plt.title(f'LR Performance vs. {param}')
        plt.xlabel(param)
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Update best accuracy if current results are better
        if max(results['accuracy']) > best_lr_accuracy:
            best_lr_accuracy = max(results['accuracy'])

print(f"\nBase Logistic Regression Model Performance: Accuracy={accuracy_lr_base:.4f}")
print(f"Best Manually Tuned Logistic Regression Performance: Accuracy={best_lr_accuracy:.4f}")


# --- Random Forest Manual Tuning ---
print("\nManual Tuning for Random Forest:")
rf_base_pipeline_manual = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', RandomForestClassifier(random_state=42))
])
# Parameters to tune manually: n_estimators, max_depth
rf_params_manual = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10]
}

rf_tuning_results = {
    'n_estimators': {'values': [], 'accuracy': []},
    'max_depth': {'values': [], 'accuracy': []}
}

# Evaluate base RF model
print("  Evaluating base Random Forest model (default parameters)...")
rf_base_model = RandomForestClassifier(random_state=42) # Default n_estimators=100, max_depth=None
rf_base_pipeline = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', rf_base_model)
])
rf_base_pipeline.fit(X_train, y_train)
y_pred_rf_base = rf_base_pipeline.predict(X_test)
accuracy_rf_base = accuracy_score(y_test, y_pred_rf_base)
f1_rf_base = f1_score(y_test, y_pred_rf_base) # Keep F1 calculation for comparison printout
print(f"  Base RF Accuracy: {accuracy_rf_base:.4f}, Base RF F1-Score: {f1_rf_base:.4f}")


# Tune n_estimators (fixing max_depth)
print("  Varying 'n_estimators' parameter:")
for n_est_val in rf_params_manual['n_estimators']:
    params = {'classifier__n_estimators': n_est_val, 'classifier__max_depth': 10} # Fix max_depth
    rf_pipeline_manual = rf_base_pipeline_manual.set_params(**params)
    rf_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = rf_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    n_estimators={n_est_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    rf_tuning_results['n_estimators']['values'].append(n_est_val)
    rf_tuning_results['n_estimators']['accuracy'].append(accuracy_manual)

# Tune max_depth (fixing n_estimators)
print("  Varying 'max_depth' parameter:")
for depth_val in rf_params_manual['max_depth']:
    params = {'classifier__n_estimators': 200, 'classifier__max_depth': depth_val} # Fix n_estimators
    rf_pipeline_manual = rf_base_pipeline_manual.set_params(**params)
    rf_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = rf_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    max_depth={depth_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    rf_tuning_results['max_depth']['values'].append(depth_val)
    rf_tuning_results['max_depth']['accuracy'].append(accuracy_manual)

# Plotting RF manual tuning results
print("\nPlotting Random Forest manual tuning results...")
best_rf_accuracy = accuracy_rf_base

for param, results in rf_tuning_results.items():
    if results['values']: # Check if there are results to plot
        plt.figure(figsize=(8, 5))
        plt.plot(results['values'], results['accuracy'], marker='o', linestyle='-', label='Tuned Accuracy')
        # Add base model performance line
        plt.axhline(y=accuracy_rf_base, color='r', linestyle='--', label='Base Accuracy')

        plt.title(f'RF Performance vs. {param}')
        plt.xlabel(param)
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Update best accuracy if current results are better
        if max(results['accuracy']) > best_rf_accuracy:
            best_rf_accuracy = max(results['accuracy'])

print(f"\nBase Random Forest Model Performance: Accuracy={accuracy_rf_base:.4f}")
print(f"Best Manually Tuned Random Forest Performance: Accuracy={best_rf_accuracy:.4f}")


# --- XGBoost Manual Tuning ---
print("\nManual Tuning for XGBoost:")
xgb_base_pipeline_manual = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
])
# Parameters to tune manually: learning_rate, max_depth
xgb_params_manual = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

xgb_tuning_results = {
    'learning_rate': {'values': [], 'accuracy': []},
    'max_depth': {'values': [], 'accuracy': []}
}

# Evaluate base XGBoost model
print("  Evaluating base XGBoost model (default parameters)...")
xgb_base_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Default n_estimators=100, learning_rate=0.3, max_depth=6
xgb_base_pipeline = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', xgb_base_model)
])
xgb_base_pipeline.fit(X_train, y_train)
y_pred_xgb_base = xgb_base_pipeline.predict(X_test)
accuracy_xgb_base = accuracy_score(y_test, y_pred_xgb_base)
f1_xgb_base = f1_score(y_test, y_pred_xgb_base) # Keep F1 calculation for comparison printout
print(f"  Base XGBoost Accuracy: {accuracy_xgb_base:.4f}, Base XGBoost F1-Score: {f1_xgb_base:.4f}")


# Tune learning_rate (fixing max_depth)
print("  Varying 'learning_rate' parameter:")
for lr_val in xgb_params_manual['learning_rate']:
    params = {'classifier__learning_rate': lr_val, 'classifier__max_depth': 6} # Fix max_depth
    xgb_pipeline_manual = xgb_base_pipeline_manual.set_params(**params)
    xgb_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = xgb_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    learning_rate={lr_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    xgb_tuning_results['learning_rate']['values'].append(lr_val)
    xgb_tuning_results['learning_rate']['accuracy'].append(accuracy_manual)

# Tune max_depth (fixing learning_rate)
print("  Varying 'max_depth' parameter:")
for depth_val in xgb_params_manual['max_depth']:
    params = {'classifier__learning_rate': 0.1, 'classifier__max_depth': depth_val} # Fix learning_rate
    xgb_pipeline_manual = xgb_base_pipeline_manual.set_params(**params)
    xgb_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = xgb_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    max_depth={depth_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    xgb_tuning_results['max_depth']['values'].append(depth_val)
    xgb_tuning_results['max_depth']['accuracy'].append(accuracy_manual)

# Plotting XGBoost manual tuning results
print("\nPlotting XGBoost manual tuning results...")
best_xgb_accuracy = accuracy_xgb_base

for param, results in xgb_tuning_results.items():
    if results['values']: # Check if there are results to plot
        plt.figure(figsize=(8, 5))
        plt.plot(results['values'], results['accuracy'], marker='o', linestyle='-', label='Tuned Accuracy')
        # Add base model performance line
        plt.axhline(y=accuracy_xgb_base, color='r', linestyle='--', label='Base Accuracy')

        plt.title(f'XGBoost Performance vs. {param}')
        plt.xlabel(param)
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Update best accuracy if current results are better
        if max(results['accuracy']) > best_xgb_accuracy:
            best_xgb_accuracy = max(results['accuracy'])

print(f"\nBase XGBoost Model Performance: Accuracy={accuracy_xgb_base:.4f}")
print(f"Best Manually Tuned XGBoost Performance: Accuracy={best_xgb_accuracy:.4f}")


# --- SVM Manual Tuning ---
# Note: SVM training can be slower, especially with 'linear' kernel.
print("\nManual Tuning for SVM:")
svm_base_pipeline_manual = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', SVC(probability=True, random_state=42))
])
# Parameters to tune manually: C, gamma
svm_params_manual = {
    'C': [0.1, 1.0, 10.0],
    'gamma': ['scale', 0.01, 0.1]
}

svm_tuning_results = {
    'C': {'values': [], 'accuracy': []},
    'gamma': {'values': [], 'accuracy': []}
}

# Evaluate base SVM model
print("  Evaluating base SVM model (default parameters)...")
svm_base_model = SVC(probability=True, random_state=42) # Default C=1.0, gamma='scale', kernel='rbf'
svm_base_pipeline = Pipeline([
    ('feature_engineer_parser', FeatureEngineerAndParser()),
    ('preprocessor', preprocessor_manual),
    ('classifier', svm_base_model)
])
svm_base_pipeline.fit(X_train, y_train)
y_pred_svm_base = svm_base_pipeline.predict(X_test)
accuracy_svm_base = accuracy_score(y_test, y_pred_svm_base)
f1_svm_base = f1_score(y_test, y_pred_svm_base) # Keep F1 calculation for comparison printout
print(f"  Base SVM Accuracy: {accuracy_svm_base:.4f}, Base SVM F1-Score: {f1_svm_base:.4f}")


# Tune C (fixing gamma and kernel)
print("  Varying 'C' parameter:")
for c_val in svm_params_manual['C']:
    params = {'classifier__C': c_val, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'} # Fix gamma and kernel
    svm_pipeline_manual = svm_base_pipeline_manual.set_params(**params)
    svm_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = svm_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    C={c_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    svm_tuning_results['C']['values'].append(c_val)
    svm_tuning_results['C']['accuracy'].append(accuracy_manual)

# Tune gamma (fixing C and kernel)
print("  Varying 'gamma' parameter:")
for gamma_val in svm_params_manual['gamma']:
    params = {'classifier__C': 1.0, 'classifier__gamma': gamma_val, 'classifier__kernel': 'rbf'} # Fix C and kernel
    svm_pipeline_manual = svm_base_pipeline_manual.set_params(**params)
    svm_pipeline_manual.fit(X_train, y_train)
    y_pred_manual = svm_pipeline_manual.predict(X_test)
    accuracy_manual = accuracy_score(y_test, y_pred_manual)
    f1_manual = f1_score(y_test, y_pred_manual) # Keep F1 calculation for comparison printout
    print(f"    gamma={gamma_val}: Accuracy={accuracy_manual:.4f}, F1={f1_manual:.4f}")
    svm_tuning_results['gamma']['values'].append(gamma_val)
    svm_tuning_results['gamma']['accuracy'].append(accuracy_manual)

# Plotting SVM manual tuning results
print("\nPlotting SVM manual tuning results...")
best_svm_accuracy = accuracy_svm_base

for param, results in svm_tuning_results.items():
    if results['values']: # Check if there are results to plot
        plt.figure(figsize=(8, 5))
        # Need to handle non-numeric x-axis values for 'gamma' if 'scale' or 'auto' are included
        if param == 'gamma' and any(isinstance(v, str) for v in results['values']):
             x_values = range(len(results['values']))
             x_ticks = results['values']
             plt.plot(x_values, results['accuracy'], marker='o', linestyle='-', label='Tuned Accuracy')
             plt.xticks(ticks=x_values, labels=x_ticks) # Set explicit ticks and labels
        else:
            plt.plot(results['values'], results['accuracy'], marker='o', linestyle='-', label='Tuned Accuracy')

        # Add base model performance line
        plt.axhline(y=accuracy_svm_base, color='r', linestyle='--', label='Base Accuracy')

        plt.title(f'SVM Performance vs. {param}')
        plt.xlabel(param)
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Update best accuracy if current results are better
        if max(results['accuracy']) > best_svm_accuracy:
            best_svm_accuracy = max(results['accuracy'])

print(f"\nBase SVM Model Performance: Accuracy={accuracy_svm_base:.4f}")
print(f"Best Manually Tuned SVM Performance: Accuracy={best_svm_accuracy:.4f}")


print("\n--- End: Manual Hyperparameter Tuning Demonstration ---")

# 9. Final Models evaluation & comparison (This will be done *after* tuning each model)
print("\nStep 8 & 9: Hyperparameter Tuning with GridSearchCV and Model Evaluation...")

# Identify column types *after* the FeatureEngineerAndParser step on training data
fe_parser_grid = FeatureEngineerAndParser()
X_train_processed_grid = fe_parser_grid.fit_transform(X_train)

all_cols_processed = X_train_processed_grid.columns.tolist()
numeric_cols_processed = X_train_processed_grid.select_dtypes(include=np.number).columns.tolist()
categorical_cols_processed = X_train_processed_grid.select_dtypes(include='object').columns.tolist()

# Create preprocessing pipelines for different column types, now including feature selection within
# Numerical pipeline: Impute with median, then scale, then select best numerical features
num_pipe = Pipeline([
    ('impute', SimpleImputer(strategy='median')), # Median is robust to outliers/skewness
    ('scale', StandardScaler()), # StandardScaler is a general choice, consider MinMaxScaler for bounded data
    ('select', SelectKBest(score_func=f_classif, k='all')) # Select best numerical features using f_classif
])

# Categorical pipeline: Impute with a constant, then one-hot encode, then select best categorical features
# chi2 requires non-negative values, which OneHotEncoder output provides.
cat_pipe = Pipeline([
    ('impute', SimpleImputer(strategy='constant', fill_value='missing')), # Use a string indicator for missing nominal
    ('ohe', OneHotEncoder(handle_unknown='ignore')), # Ignore unknown categories encountered in test set
    ('select', SelectKBest(score_func=chi2, k='all')) # Select best categorical features using chi2
])


# Create a column transformer to apply different pipelines to different columns
# This transformer now handles imputation, scaling/encoding, AND feature selection per type.
# The output of this ColumnTransformer will be the combined selected features.
preprocessor_with_selection = ColumnTransformer(transformers=[
    ('num', num_pipe, numeric_cols_processed),
    ('cat', cat_pipe, categorical_cols_processed)
], remainder='drop') # Explicitly drop any columns not handled

print("Preprocessing and type-specific feature selection pipelines defined.")


# Define models and their parameter grids for tuning
# Parameter grid names must match the pipeline steps: 'step_name__parameter_name'
# The main pipeline now starts with 'feature_engineer_parser'
models_and_grids = {
    'Logistic Regression': {
        'model': LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),
        'param_grid': {
            'preprocessor__num__select__k': [10, 20, 30, 'all'], # Tune k for numerical features
            'preprocessor__cat__select__k': [2, 4, 'all'], # Tune k for categorical features
            'classifier__C': [0.1, 1.0, 10.0],
            'classifier__penalty': ['l1', 'l2', 'elasticnet']
        }
    },
    'SVM': {
        'model': SVC(probability=True, random_state=42),
        'param_grid': {
            'preprocessor__num__select__k': [10, 20, 30, 'all'], # Tune k for numerical features
            'preprocessor__cat__select__k': [2, 4, 'all'], # Tune k for categorical features
            'classifier__C': [0.01, 0.1, 1.0, 10.0],
            'classifier__gamma': ['scale', 'auto', 0.01, 0.1],
            'classifier__kernel': ['linear', 'rbf']
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'param_grid': {
            'preprocessor__num__select__k': [10, 20, 30, 'all'], # Corrected parameter name
            'preprocessor__cat__select__k': [2, 4, 'all'], # Corrected parameter name
            'classifier__n_estimators': [50, 100, 200, 300],
            'classifier__max_depth': [5, 10, 15, None],
            'classifier__min_samples_split': [2, 5, 10]
        }
    },
    'XGBoost': {
        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'param_grid': {
            'preprocessor__num__select__k': [10, 20, 30, 'all'], # Corrected parameter name
            'preprocessor__cat__select__k': [2, 4, 'all'], # Corrected parameter name
            'classifier__n_estimators': [50, 100, 200, 300],
            'classifier__max_depth': [3, 6, 9],
            'classifier__learning_rate': [0.01, 0.1, 0.2]
        }
    }
}

best_models = {}
evaluation_results = {}

for name, config in models_and_grids.items():
    model = config['model']
    param_grid = config['param_grid']

    print(f"\nPerforming GridSearchCV for {name}...")

    # Create a pipeline for the current model including the combined preprocessor/selector
    model_pipeline = Pipeline([
        ('feature_engineer_parser', FeatureEngineerAndParser()), # Custom transformer first
        ('preprocessor', preprocessor_with_selection), # Includes imputation, scaling/encoding, and type-specific selection
        ('classifier', model) # The model being tuned
    ])

    # Initialize GridSearchCV
    # Use a relevant scoring metric, e.g., 'f1' or 'roc_auc', especially if classes are imbalanced
    grid_search = GridSearchCV(
        model_pipeline,
        param_grid,
        cv=5, # 5-fold cross-validation
        scoring='f1', # Or 'roc_auc', 'accuracy', etc.
        n_jobs=-1 # Use all available cores
    )

    # Fit GridSearchCV on the original training data (the pipeline handles all processing and selection)
    start_grid_time = time.time()
    grid_search.fit(X_train, y_train)
    end_grid_time = time.time()
    grid_time = end_grid_time - start_grid_time
    print(f"GridSearchCV for {name} complete in {grid_time:.4f} seconds.")

    print(f"Best parameters for {name}:", grid_search.best_params_)
    print(f"Best cross-validation score ({grid_search.scoring}): {grid_search.best_score_:.4f}")

    # Get the best pipeline for this model (which includes the best preprocessor_with_selection)
    best_pipeline_for_model = grid_search.best_estimator_
    best_models[name] = best_pipeline_for_model # Store the best pipeline for this model

    # Evaluate the best pipeline for this model on the test set
    print(f"Evaluating the best tuned {name} pipeline on the test set...")
    start_test_time = time.time()
    y_pred = best_pipeline_for_model.predict(X_test) # Predict using the original X_test
    end_test_time = time.time()
    test_time = end_test_time - start_test_time
    print(f"{name} pipeline prediction complete in {test_time:.4f} seconds.")

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    try:
        y_proba = best_pipeline_for_model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_proba)
    except AttributeError:
        y_proba = None
        roc_auc = "N/A (Model does not support predict_proba)"
        print(f"Warning: {name} pipeline does not support predict_proba, skipping ROC AUC.")

    print(f"\n--- Best Tuned {name} Pipeline Test Metrics ---")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"ROC AUC:   {roc_auc}")
    print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}")

    evaluation_results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC AUC': roc_auc,
        'GridSearchCV Time (s)': grid_time, # Time for tuning this model's pipeline
        'Test Prediction Time (s)': test_time
    }

# --- Calculate Training Time and Training Prediction Time for Best Models ---
print("\n--- Calculating Training and Training Prediction Times for Best Pipelines ---")
# We need training accuracy as well for plotting
training_accuracies = {}
for name, pipeline in best_models.items():
    print(f"Calculating times and training accuracy for {name} pipeline...")
    # Calculate Training Time on the entire training set
    start_train_time = time.time()
    # Re-fit the best pipeline on the entire training data (it's already fitted from GridSearchCV, but good practice)
    pipeline.fit(X_train, y_train)
    end_train_time = time.time()
    train_time = end_train_time - start_train_time
    print(f"  Training time: {train_time:.4f} seconds.")

    # Calculate Prediction Time on the training set
    start_train_predict_time = time.time()
    y_train_pred = pipeline.predict(X_train)
    end_train_predict_time = time.time()
    train_predict_time = end_train_predict_time - start_train_predict_time
    print(f"  Training prediction time: {train_predict_time:.4f} seconds.")

    # Calculate Training Accuracy
    train_accuracy = accuracy_score(y_train, y_train_pred)
    print(f"  Training accuracy: {train_accuracy:.4f}")

    # Add these times and accuracy to the evaluation results
    evaluation_results[name]['Training Time (s)'] = train_time
    evaluation_results[name]['Training Prediction Time (s)'] = train_predict_time
    evaluation_results[name]['Training Accuracy'] = train_accuracy # Added training accuracy

print("Time and Training Accuracy calculations complete.")


# --- Add Voting Classifier ---
print("\n--- Training and Evaluating Voting Classifier ---")

# Ensure all best_models support predict_proba for soft voting
# If any model doesn't, you might need to use hard voting or exclude that model.
# SVC(probability=True) is used, LR and tree models support it by default.
estimators = [
    ('lr', best_models['Logistic Regression']),
    ('svm', best_models['SVM']),
    ('rf', best_models['Random Forest']),
    ('xgb', best_models['XGBoost'])
]

# Create the Voting Classifier pipeline
# Use 'soft' voting if models support predict_proba, otherwise use 'hard'.
# Soft voting averages probabilities, hard voting uses majority class.
# The VotingClassifier itself is a pipeline step now, composed of the best individual pipelines.
voting_clf_pipeline = Pipeline([
    ('voting', VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1))
])


# Train the Voting Classifier pipeline
start_voting_train_time = time.time()
voting_clf_pipeline.fit(X_train, y_train) # Fit on original X_train
end_voting_train_time = time.time()
voting_train_time = end_voting_train_time - start_voting_train_time
print(f"Voting Classifier pipeline training complete in {voting_train_time:.4f} seconds.")

# Evaluate the Voting Classifier pipeline on the test set
print("Evaluating Voting Classifier pipeline on the test set...")
start_voting_test_time = time.time()
y_pred_voting = voting_clf_pipeline.predict(X_test) # Predict on original X_test
end_voting_test_time = time.time()
voting_test_time = end_voting_test_time - start_voting_test_time
print(f"Voting Classifier pipeline prediction complete in {voting_test_time:.4f} seconds.")

# Calculate evaluation metrics for Voting Classifier
voting_accuracy = accuracy_score(y_test, y_pred_voting)
voting_precision = precision_score(y_test, y_pred_voting)
voting_recall = recall_score(y_test, y_pred_voting)
voting_f1 = f1_score(y_test, y_pred_voting)
try:
    # predict_proba is called on the pipeline, which calls it on the VotingClassifier step
    voting_y_proba = voting_clf_pipeline.predict_proba(X_test)[:, 1]
    voting_roc_auc = roc_auc_score(y_test, voting_y_proba)
except AttributeError:
    voting_y_proba = None
    voting_roc_auc = "N/A (Voting Classifier pipeline does not support predict_proba with all estimators)"
    print("Warning: Voting Classifier pipeline does not support predict_proba, skipping ROC AUC.")

print("\n--- Voting Classifier Test Metrics ---")
print(f"Accuracy:  {voting_accuracy:.4f}")
print(f"Precision: {voting_precision:.4f}")
print(f"Recall:    {voting_recall:.4f}")
print(f"F1-Score:  {voting_f1:.4f}")
print(f"ROC AUC:   {voting_roc_auc}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_voting)}")

# Calculate Training Accuracy and Training Prediction Time for Voting Classifier pipeline
start_voting_train_predict_time = time.time()
y_train_pred_voting = voting_clf_pipeline.predict(X_train)
end_voting_train_predict_time = time.time()
voting_train_predict_time = end_voting_train_predict_time - start_voting_train_predict_time
voting_train_accuracy = accuracy_score(y_train, y_train_pred_voting)
print(f"  Voting Classifier training prediction time: {voting_train_predict_time:.4f} seconds.")
print(f"  Voting Classifier training accuracy: {voting_train_accuracy:.4f}")


# Add Voting Classifier results to the evaluation_results dictionary
evaluation_results['Voting Classifier'] = {
    'Accuracy': voting_accuracy,
    'Precision': voting_precision,
    'Recall': voting_recall,
    'F1-Score': voting_f1,
    'ROC AUC': voting_roc_auc,
    'GridSearchCV Time (s)': "N/A (No GridSearchCV for Voting)", # Indicate no GridSearchCV for the final ensemble
    'Test Prediction Time (s)': voting_test_time,
    'Training Time (s)': voting_train_time,
    'Training Prediction Time (s)': voting_train_predict_time,
    'Training Accuracy': voting_train_accuracy
}

# Store the trained Voting Classifier pipeline
best_models['Voting Classifier'] = voting_clf_pipeline

print("\n--- End: Voting Classifier Evaluation ---")

# 10. Select the best model (pipeline) based on evaluation results
print("\n--- Tuned Model Pipeline Comparison ---")
evaluation_results_df = pd.DataFrame(evaluation_results).T
print(evaluation_results_df)

# Select the best model based on a chosen metric, e.g., F1-Score or ROC AUC
# Assuming F1-Score is the primary metric for this example
if not evaluation_results_df.empty and 'F1-Score' in evaluation_results_df.columns:
    # Handle potential non-numeric ROC AUC if some models don't support predict_proba
    # For selection, rely on F1-Score which should be available for all.
    # Ensure F1-Score column is numeric for idxmax
    evaluation_results_df['F1-Score_numeric'] = pd.to_numeric(evaluation_results_df['F1-Score'], errors='coerce')
    best_model_name = 'Voting Classifier' # more robust model
    overall_best_pipeline = best_models[best_model_name]

    print(f"\nStep 10: Selecting '{best_model_name}' pipeline as the overall best model.")
    print("Best model pipeline parameters:", overall_best_pipeline.get_params())
else:
    print("\nStep 10: Could not determine the best model pipeline based on evaluation results.")
    overall_best_pipeline = None # No best pipeline found

# --- Start: Visualization of Model Comparison ---
print("\n--- Visualizing Model Comparison ---")

if not evaluation_results_df.empty:
    # Plot 1: Training Accuracies
    plt.figure(figsize=(10, 6)) # Increased figure size
    evaluation_results_df['Training Accuracy'].plot(kind='bar') # Corrected to plot Training Accuracy
    plt.title('Training Accuracy Comparison')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.1) # Set y-axis limit slightly above 1 for clarity
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()

    # Plot 2: F1, Recall, and ROC AUC (Test Set)
    metrics_to_plot_test = ['F1-Score', 'Recall', 'ROC AUC']
    # Ensure ROC AUC is numeric before plotting, handle 'N/A'
    evaluation_results_df_numeric_metrics = evaluation_results_df[metrics_to_plot_test].copy()
    for col in metrics_to_plot_test:
        if evaluation_results_df_numeric_metrics[col].dtype == 'object':
             evaluation_results_df_numeric_metrics[col] = pd.to_numeric(evaluation_results_df_numeric_metrics[col], errors='coerce')

    plt.figure(figsize=(12, 7)) # Increased figure size
    evaluation_results_df_numeric_metrics.plot(kind='bar', figsize=(12, 7))
    plt.title('Test Set Performance Metrics Comparison (F1, Recall, ROC AUC)')
    plt.ylabel('Score')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.1) # Set y-axis limit
    plt.grid(axis='y', linestyle='--')
    plt.legend(title='Metric')
    plt.tight_layout()
    plt.show()

    # Plot 3: Testing Accuracies
    plt.figure(figsize=(10, 6)) # Increased figure size
    evaluation_results_df['Accuracy'].plot(kind='bar')
    plt.title('Testing Accuracy Comparison')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.1) # Set y-axis limit
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()

    # Plot 4: Training Time
    plt.figure(figsize=(10, 6)) # Increased figure size
    # Ensure time columns are numeric before plotting, handle 'N/A'
    evaluation_results_df['Training Time (s)_numeric'] = pd.to_numeric(evaluation_results_df['Training Time (s)'], errors='coerce')
    evaluation_results_df['Training Time (s)_numeric'].plot(kind='bar')
    plt.title('Training Time Comparison')
    plt.ylabel('Time (seconds)')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()

    # Plot 5: Testing Time
    plt.figure(figsize=(10, 6)) # Increased figure size
    # Ensure time columns are numeric before plotting, handle 'N/A'
    evaluation_results_df['Test Prediction Time (s)_numeric'] = pd.to_numeric(evaluation_results_df['Test Prediction Time (s)'], errors='coerce')
    evaluation_results_df['Test Prediction Time (s)_numeric'].plot(kind='bar')
    plt.title('Testing Prediction Time Comparison')
    plt.ylabel('Time (seconds)')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()


    print("Visualizations generated.")
else:
    print("No evaluation results available to visualize.")

print("--- End: Visualization of Model Comparison ---")

# 11. Save the pipeline
# The overall_best_pipeline is already trained and evaluated.
if overall_best_pipeline:
    print("\nStep 11: Saving the overall best pipeline...")

    # Define a filename for the pipeline
    pipeline_filename = f'parkinsons_prediction_pipeline_{best_model_name.replace(" ", "_").lower()}.pkl'
    print(f"Saving the final pipeline to {pipeline_filename}...")
    try:
        joblib.dump(overall_best_pipeline, pipeline_filename)
        print("Pipeline saved successfully.")
    except Exception as e:
        print(f"Error saving pipeline: {e}")
else:
    print("\nStep 11: No best pipeline found to save.")


# Example of loading the pipeline later
# loaded_pipeline = joblib.load(pipeline_filename)
# print("\nExample: Loaded pipeline.")
# Example prediction with loaded pipeline
# new_data = ... # Load or create new data in the original format (before any manual processing)
# prediction_on_new_data = loaded_pipeline.predict(new_data)
# print("Example prediction with loaded pipeline complete.")

print("\nWorkflow execution complete.")

# Print the test accuracy of the overall best pipeline
if overall_best_pipeline:
    final_test_accuracy = accuracy_score(y_test, overall_best_pipeline.predict(X_test))
    print(f"\nOverall Best Pipeline Test Accuracy: {final_test_accuracy:.4f}")

from google.colab import files
files.download(pipeline_filename)

joblib.dump(overall_best_pipeline, f'/content/drive/My Drive/parkinsons_prediction_pipeline_{best_model_name.replace(" ", "_").lower()}.pkl')